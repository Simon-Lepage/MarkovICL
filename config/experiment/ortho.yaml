# @package _global_

data: 
  training_matrices: -1
  use_remap: false

train:
  total_steps: 20000

model:
  _target_: src.llama_wrappers.OrthogonalLlama
  config: 
    _target_: transformers.models.llama.LlamaConfig
    vocab_size: 0
    hidden_size: 256
    intermediate_size: 256
    num_hidden_layers: 4
    num_attention_heads: 2
    max_position_embeddings: ${data.walk_len}
    tie_word_embeddings: True # Not really useful, vocab size is 0.
    attention_dropout: 0.

exp_name: ortho